{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP Spam Classifier.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8jq0aTTCstFj","colab_type":"text"},"source":["# Spam Dectector"]},{"cell_type":"code","metadata":{"id":"Sb_0JPXcrijp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"outputId":"f3cad6f7-e71a-49fa-feb1-a70defea27af","executionInfo":{"status":"ok","timestamp":1572036934225,"user_tz":-60,"elapsed":1507,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGo6aIm0tOcd5EhqWlYb0rime9sBvHS9YMpx0D2w=s64","userId":"08597265227091462140"}}},"source":["import numpy as np\n","import pandas as pd\n","\n","file_name = \"https://raw.githubusercontent.com/rajeevratan84/datascienceforbusiness/master/spam.csv\"\n","data = pd.read_csv(file_name, encoding = \"latin-1\")\n","data.head()\n","data = data[['v1', 'v2']]\n","data = data.rename(columns = {'v1': 'label', 'v2': 'text'})\n","data.head()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                               text\n","0   ham  Go until jurong point, crazy.. Available only ...\n","1   ham                      Ok lar... Joking wif u oni...\n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n","3   ham  U dun say so early hor... U c already then say...\n","4   ham  Nah I don't think he goes to usf, he lives aro..."]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"7H_Uz4xMsWBu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"e1b6f1b1-f5c9-4d97-9492-0ccec178125a","executionInfo":{"status":"ok","timestamp":1572036934229,"user_tz":-60,"elapsed":1169,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGo6aIm0tOcd5EhqWlYb0rime9sBvHS9YMpx0D2w=s64","userId":"08597265227091462140"}}},"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk import pos_tag, word_tokenize\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import svm\n","from sklearn.metrics import confusion_matrix\n","\n","lemmatizer = WordNetLemmatizer()\n","stopwords = set(stopwords.words('english'))\n","\n","def review_messages(msg):\n","    # converting messages to lowercase\n","    msg = msg.lower()\n","    return msg\n","\n","def alternative_review_messages(msg):\n","    # converting messages to lowercase\n","    msg = msg.lower()\n","\n","    # uses a lemmatizer (wnpos is the parts of speech tag)\n","    # unfortunately wordnet and nltk uses a different set of terminology for pos tags\n","    # first, we must translate the nltk pos to wordnet\n","    nltk_pos = [tag[1] for tag in pos_tag(word_tokenize(msg))]\n","    msg = [tag[0] for tag in pos_tag(word_tokenize(msg))]\n","    wnpos = ['a' if tag[0] == 'J' else tag[0].lower() if tag[0] in ['N', 'R', 'V'] else 'n' for tag in nltk_pos]\n","    msg = \" \".join([lemmatizer.lemmatize(word, wnpos[i]) for i, word in enumerate(msg)])\n","\n","    # removing stopwords \n","    msg = [word for word in msg.split() if word not in stopwords]\n","\n","    return msg"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bNeYbeGnsYu_","colab_type":"code","colab":{}},"source":["# Processing text messages\n","data['text'] = data['text'].apply(review_messages)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FF2dHCXQsM4F","colab_type":"code","colab":{}},"source":["# train test split \n","X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size = 0.1, random_state = 1)\n","\n","# training vectorizer\n","vectorizer = TfidfVectorizer()\n","X_train_vec = vectorizer.fit_transform(X_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPlaxorAsa2E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"9ead5170-89ce-4b9c-a3ac-8e20c4064ef2","executionInfo":{"status":"ok","timestamp":1572036935708,"user_tz":-60,"elapsed":1719,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGo6aIm0tOcd5EhqWlYb0rime9sBvHS9YMpx0D2w=s64","userId":"08597265227091462140"}}},"source":["# training the classifier \n","svm = svm.SVC(C=1000)\n","svm.fit(X_train_vec, y_train)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n","    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n","    shrinking=True, tol=0.001, verbose=False)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"icHBwnywscLX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"d1d8b0c4-9ef5-4a4d-8a30-7f7baf50ceb6","executionInfo":{"status":"ok","timestamp":1572036935710,"user_tz":-60,"elapsed":1383,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGo6aIm0tOcd5EhqWlYb0rime9sBvHS9YMpx0D2w=s64","userId":"08597265227091462140"}}},"source":["# testing against testing set \n","X_test = vectorizer.transform(X_test)\n","y_pred = svm.predict(X_test) \n","print(confusion_matrix(y_test, y_pred))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["[[490   0]\n"," [ 10  58]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u0Qpz_S0sdJ9","colab_type":"code","colab":{}},"source":["# test against new messages \n","def pred(msg):\n","    msg = vectorizer.transform([msg])\n","    prediction = svm.predict(msg)\n","    return prediction[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6O7v-wnrdcq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"52682b33-df23-47c5-89a1-0c02c5ac7f0c","executionInfo":{"status":"ok","timestamp":1572037728008,"user_tz":-60,"elapsed":522,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGo6aIm0tOcd5EhqWlYb0rime9sBvHS9YMpx0D2w=s64","userId":"08597265227091462140"}}},"source":["rand_index = np.random.randint(0, len(data))\n","test_sample = data.iloc[rand_index][1]\n","print(test_sample)\n","print(\"Text is - \" + str(pred(test_sample)))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["i'm on the bus. love you\n","Text is - ham\n"],"name":"stdout"}]}]}